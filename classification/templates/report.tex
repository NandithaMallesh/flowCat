\documentclass[11pt,a4paper]{article}
\usepackage[margin=2cm]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{pdflscape}

\begin{document}


\section{SOM Generation setup}

Result analysis have been done for the included cohort using the following settings in SOM-Generation:

\begin{description}
   \item[normal] SOM generated using 5 cases from each cohort, without any further modification of the data
   \item[pregated] Pregating of each individual SOM on negative side-scatter and positive CD45, which roughly translates to the lymphocyte population
\end{description}

Some additional modifications of the dataset for the consensus SOM generation can be applied in both approaches.

\begin{description}
   \item[exc] Exclusion of the normal cohort in the reference SOM generation
   \item[selected] Cases for reference SOM are preselected instead of randomly chosen from each cohort
\end{description}

This yields us 4 different configurations for the initial consensus SOM generation and followed upsampling.

Additionally some additional SOM processing methods have been tested in another selection of setups.

\begin{description}
   \item[som] Consensus SOM created using SOM nodes instead of raw data.
   \item[somgated] Consensus SOM created using SOM nodes after pregating.
   \item[som\_combined] SOM nodes will be used for combined gating.
   \item[pregated\_combined] Pregating for consensus SOM batched for all input cases.
   \item[always\_som] SOM nodes for both consensus SOM and also upsampling of individual samples.
\end{description}

After our initial AWS Batch runs of the consensus SOM samples, we have obtained the following number of upsampled data.

\begin{figure}[h]
   \centering
   \includegraphics[width=1\textwidth]{experiment_overview}
   \caption{Overview of number of generated experiments per type.}
\end{figure}

Two sets of experiments have been generated so far, with \emph{initial\_comp} containing the first set of experiments and \emph{comp\_pregating}, containing additional experiments with other SOM-generation settings.


\section{Classification setup}

Each setup is classified independently and afterwards identical replications are averaged to provide the average classification
accuracy over multiple consensus SOM generations.

In classification there are also multiple parameters, which we can vary, of which the most important are:

\begin{description}
   \item[groups] Diagnoses to be classified, which can also be grouped into new meta-classes, that count as one
   \item[number] Maximum number of cases classified for each group. Alternatively all classes can be reduced to the lowest one.
\end{description}

The random component in the classifier is constrained to the initialization of the neural network weights and the choice of cases in the train and test datasets.

\begin{description}
   \item[initial\_comp] Comparisons between LMg (LPL, Marginal), MtCp (Mantle, CLL/PL), CM (CLL, MBL) and normal with maximum group size of 2000.
\end{description}

This setup is applied to all experiments described in the SOM setup section.


\section{Classification results}

Analysis of classification results has been initially done in the classification procedure itself, which produced a result json file containing individual run statistics and also averaged statistics over all runs.
This approach lacked flexibility in the analysis approach and did not lend itself well to more complex interpretations of the prediction data, such as top N accuracies, introduction of uncertainty classes and changes in the application and calculation of averages in these approaches.

By directly saving the output layer activations, we can postpone analysis to a later stage. Giving us more flexibility in the interpretation.

\begin{table}[h]
   \centering
   \input{result.tex}
   \caption{Classification results using the different configurations of SOM input files.}
\end{table}

\begin{landscape}
\begin{table}
   \resizebox{\linewidth}{!}{%
   \input{prediction_meta.tex}
   }
   \caption{Classification result metadata.}
\end{table}
\end{landscape}

\section{Classification analysis and prediction interpretation}

In analyzing the results of the classification we want to answer a few basic questions:

\begin{itemize}
   \item consensus SOM
      \begin{itemize}
         \item Which preprocessing steps improve classification results to what extent?
         \item How does group composition of the SOM affect classification results?
            \begin{itemize}
               \item How many cases in the consensus SOM is optimal?
               \item Does inclusion/exclusion of the normal cohort affect the result?
               \item How much does case quality influence the quality of classification, does infiltration matter in the selection of the cases?
            \end{itemize}
      \end{itemize}
   \item Classification
      \begin{itemize}
         \item Grouping of different entities into combined classes?
         \item Adaptations to the neural network classifier?
         \item Implementation of alternative classifiers?
      \end{itemize}
   \item Prediction interpretation
      \begin{itemize}
         \item Analysis of top-N (especially 2) accuracy as an alternative? (required special treatment of the normal diagnosis)
         \item thresholds for misclassifications, can we solve a subset with very high certainty? Creation of an uncertain class
         \item Generalization of this idea in ROC analysis of the results.
      \end{itemize}
   \item Misclassification analysis
      \begin{itemize}
         \item Identify often misclassified cases.
         \item Identify cases misclassified with very high certainty.
      \end{itemize}
\end{itemize}

\end{document}
